{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our deep learning model, a Transformer, is to analyze game reviews and classify the sentiments associated with these reviews.  \n",
    "To train our model, we will use the \"Steam Reviews\" dataset, available at: https://www.kaggle.com/datasets/andrewmvd/steam-reviews.  \n",
    "This dataset includes, among other features, 6.4 million English reviews from the Steam platform.  \n",
    "Each review in the dataset is labeled with its sentiment: 1 for positive and -1 for negative.  \n",
    "The objective of our model is to take a game review as input and predict whether the sentiment is positive or negative.  \n",
    "To implement the model, we will use the Keras API provided by TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1/ Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the Transformer and training it with our data, it is necessary to :  \n",
    "- Load the dataset (store in .csv format in the \"datasets\" folder)\n",
    "- Seperate this dataset in train data, validation data and test data\n",
    "- Transform these initial text data into token sequences\n",
    "- Pad these sequences to ensure they have the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset importing and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_csv(\"datasets/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   app_id        app_name                                        review_text  \\\n",
      "0      10  Counter-Strike                                    Ruined my life.   \n",
      "1      10  Counter-Strike  This will be more of a ''my experience with th...   \n",
      "2      10  Counter-Strike                      This game saved my virginity.   \n",
      "3      10  Counter-Strike  • Do you like original games? • Do you like ga...   \n",
      "4      10  Counter-Strike           Easy to learn, hard to master.             \n",
      "\n",
      "   review_score  review_votes  \n",
      "0             1             0  \n",
      "1             1             1  \n",
      "2             1             0  \n",
      "3             1             0  \n",
      "4             1             1  \n"
     ]
    }
   ],
   "source": [
    "# Data overview\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews : 6417106\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviews :\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this dataset includes 6.4 million reviews, this number is too high, so it is necessary to keep only a portion of these reviews by selecting them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of the sample size\n",
    "sample_size = 100000\n",
    "\n",
    "# Randomly select review indices\n",
    "index_reviews_kept = random.sample(range(len(dataset)), sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will keep the data that interest us: review content ('review_text') and their associated sentiment ('review_score')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data by converting it into a numpy array and setting the review format to string\n",
    "X = np.array(dataset.iloc[index_reviews_kept].review_text, dtype = \"str\")\n",
    "y = np.array(dataset.iloc[index_reviews_kept].review_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 I recommend this with a proviso. It was a great game when it came out Lo These Many Years Ago, and it's still a great game and a wonderful piece of design. There also isn't really a modern equivalent which does the same things. However, on Win8 (and possibly other moderns OSs) it's quite glitchy and suffers from strange slowdowns which make it, if not unplayable, rather less fun than it could be. So, modern buyers beware.\n",
      "1 I AM SO GLAD I GOT THIS GAME. The reviews made it sound like a mediocre experience. SO WRONG. Absolutely loving this game, story is fun and interesting and the combat kicks frken ♥♥♥. Game is challenging, but rewarding and the fact you gain new abilites and weapons nearly up until the end makes it bloody hard to put down. Tonnes of replayability (including a one hit kill/death mode) and tonnes of actual content without replaying. Love the platforming that rewards skill and some of the bosses, wow, most intense and insane boss fights I've ever seen. I trully have no qualms about this game at all:)\n",
      "1 10/10 would play again.  2 damn hard 4 me tho\n"
     ]
    }
   ],
   "source": [
    "for i in range(3) :\n",
    "    print(y[i], X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sentiment value for negative sentiment to 0\n",
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we will split the data into training, validation, and test sets to train the transformer model.  \n",
    "The training set will contain 60% of the initial data, while the validation and test sets will each contain 20% of the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using the train_test_split function from scikit-learn\n",
    "X_train, X_test_full, y_train, y_test_full = train_test_split(X, y, train_size = 0.6, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = X_test_full[:int(len(X_test_full)/2)], y_test_full[:int(len(X_test_full)/2)]\n",
    "X_test, y_test = X_test_full[int(len(X_test_full)/2):], y_test_full[int(len(X_test_full)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (20000,) (20000,)\n",
      "(60000,) (20000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation of text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step consists of transforming the review's words into tokens in order to make this data readable by the AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of words for the token dictionary\n",
    "num_words = 4000\n",
    "\n",
    "# Instantiate the Tokeniser and fit it on the training data\n",
    "tokenizer = Tokenizer(num_words = num_words, oov_token = \"UNK\")\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words in the data : 66397\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of different words in the data : %d\" % len(tokenizer.word_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of occurrences for OOV words : 49\n"
     ]
    }
   ],
   "source": [
    "max_occurences_oov_words = tokenizer.word_counts[tokenizer.index_word[num_words]]\n",
    "print(\"Maximum number of occurrences for OOV words :\" , max_occurences_oov_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert sentences composed of words into sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76, 1076, 27, 174], [12, 6, 1, 203, 4, 136, 8, 1], [910, 246, 139, 37, 311, 39, 8, 2, 3740, 859, 1988, 7, 20, 112, 75, 12, 6, 9, 2013, 17, 544, 14, 119, 1213, 26, 196, 152, 57, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, it is necessary to pad these sequences : étant donné que they don't have all the same lengths (the sentences don't have all the same number of words), they can't be use by the model.  \n",
    "We will pad these sequences by cutting these which have a length superior than a setted maximum number (= \"maxlen\"), and fill the sentences which have a length inferior than maxlen with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(X_train_sequences, maxlen = 400)\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen = 400)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1076,   27,  174],\n",
       "       [   0,    0,    0, ...,  136,    8,    1],\n",
       "       [   0,    0,    0, ...,  152,   57,    8],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    1,  156,    6],\n",
       "       [   0,    0,    0, ..., 3516,    1,  130],\n",
       "       [   0,    0,    0, ...,   31,    2,  698]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
